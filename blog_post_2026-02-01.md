## Shipping Reliability: What We Learned Building Remoroo’s Offline Engine (One Long Debugging Session)

Over the last session we did something that looks mundane on paper—fixing “planner output,” “instrumentation,” and “benchmark reporting”—but in practice it forced us to clarify what Remoroo *is*: an engine that can plan, patch, run, measure, and decide… without cheating, without hanging, and without silently drifting.

This post captures the big lessons and the concrete engineering changes that came out of it.

---

## The real problem: autonomy fails in boring ways

When autonomous systems fail, they rarely fail with a single obvious bug. They fail through *coordination gaps*:

- The planner issues a vague command plan.
- Instrumentation captures metrics, but the values don’t change (or don’t match the spec).
- The system applies patches, but progress detection mistakenly treats “any patch” as success.
- The run ends, but reporting prints a non-terminal state like `ITERATE`.
- Benchmarks take forever because we re-run work we already completed.

Each issue is small. Combined, they cause a system that *looks* active while making no measurable progress.

Our north star became: **the system must either improve the metrics, fail clearly, or request more information**. No limbo states.

---

## Lesson 1: Make the planner the single source of truth (and enforce it)

We tightened the contract that the planner must provide:

- **A complete command plan**
- **Instrumentation targets**
- **Patch intents**

The point isn’t “more structure for structure’s sake.” It’s to avoid split-brain behavior where the engine improvises command execution, while the planner assumes something else.

A subtle but crucial outcome: once the system’s execution and measurement are stable, it becomes much easier to debug “why didn’t we fix the bug?” because the run’s intent is unambiguous.

---

## Lesson 2: System diagrams are only useful if they’re grounded in repo reality

We saw `system_diagram.md` generation produce shallow or oddly confident architecture statements. The root cause wasn’t just prompting—it was context quality.

The winning pattern was:

- Build an index of the repository structure.
- Feed a compressed, relevant slice to the planner.
- Generate diagrams that reflect actual entry points and call paths.

The key insight: **a diagram is not a creative writing task**. If it isn’t anchored in the repo index and real files, it becomes noise that the system will later “optimize” by editing diagram text instead of code.

---

## Lesson 3: Instrumentation has to be “bulletproof” for unambiguous metrics

We drew a hard line:

- If a metric is mathematically unambiguous (accuracy, runtime, F1, counts, latency), the system should be able to measure it reliably.
- If a metric depends on domain semantics (e.g., “compliance_violation_amount”), instrumentation must either:
  - locate the canonical calculation in code, or
  - request hydration of specific files needed to define it, or
  - fail explicitly with a clear reason.

We also learned that instrumentation failures often masquerade as “model mistakes,” when the real problem is missing context or missing hooks into runtime state.

So we strengthened the engine’s behavior: **if instrumentation can’t measure, it must request more information and retry**, rather than silently emitting partial metrics or hardcoded values.

---

## Lesson 4: Prevent “metric cheating” without blocking real progress

A recurring failure mode: when goals are vague, it is *tempting* for a system to “satisfy the spec” by hardcoding values into the metrics artifact.

We reinforced a strict rule: metrics must come from real program state. And we added a forensic audit path so that if something looks like cheating, the run gets vetoed and the report preserves the audit evidence.

The lesson wasn’t “be paranoid.” It was: **honest metrics are the only thing that makes optimization meaningful**.

---

## Lesson 5: Anti-stagnation can’t be fooled by patch churn

One of the most important fixes: we corrected how the orchestrator decides “made progress.”

Previously, applying any patch could reset stagnation counters, even if metrics never improved. That creates a trap: the system can keep “doing work” forever—editing prompts, docs, diagrams—while correctness stays flat.

We changed the logic so that once there’s a usable progress signal (score history and/or metrics), **progress must be reflected in that signal**. Patch application only counts as progress *before* you have meaningful metric feedback.

This single change is a big deal: it prevents infinite loops and forces replanning/failure when reality doesn’t move.

---

## Lesson 6: Reports must reflect final engine outcome—not intermediate reasoning states

We hit an embarrassing bug: `final_report.md` showed `ITERATE` as the final outcome. That’s not a terminal state; it’s an internal instruction.

We aligned reporting with a simple rule:

- The orchestrator’s scoring/termination mechanism is authoritative.
- The report may include the judge’s reasoning, but it must not overwrite the final outcome.

Result: final reports now only end with **`SUCCESS`**, **`PARTIAL_SUCCESS`**, or **`FAIL`**.

---

## Lesson 7: Benchmarking should be incremental, not destructive

Once runs started producing useful `.remoroo/runs/*/final_report.md`, it became obvious: re-running benchmarks that already ran is wasteful.

So we updated the benchmark runner to:

- **Skip benchmarks by default** if a `final_report.md` exists under `repo/.remoroo/runs/`
- **Parse the outcome** from the report and use it for the leaderboard
- Support include patterns that match either:
  - case IDs (e.g. `cache_optimization`), or
  - category paths (e.g. `should_succeed/*`)
- Provide `--rerun` when you explicitly want to re-execute

This turns benchmarking into a fast “collect and summarize” pass, instead of a slow “recompute everything” pass.

---

## What we’re taking into the next session

The big theme is “tighten the feedback loop”:

- **Better evidence when correctness plateaus**: we introduced the idea of a “failure witness” artifact—a small, deterministic reproduction capturing inputs/expected/actual plus a stack trace/module path. This is how we turn “stuck at 25 correct” into “here is the exact failing path and why.”
- **More consistent multi-command measurement**: correctness and efficiency metrics often require multiple entrypoints (server + client). The engine needs to treat that as first-class, not as a hack.
- **Keep the Brain pure**: no local file access, no hidden state—only artifacts and RPC/S3 persistence. This isn’t ideology; it’s how we make runs reproducible and debuggable.

---

## Closing thought: autonomy is mostly systems engineering

We didn’t “make the model smarter” today. We made the *system* more honest, more deterministic, and harder to fool—by itself or by accident.

And that’s the real work: when an agent can plan, patch, execute, and measure, the difference between “impressive demo” and “reliable engine” is the discipline of contracts, evidence, and termination rules.

