diff --git a/remoroo_monitor.py b/remoroo_monitor.py
new file mode 100644
index 0000000..e593c4a
--- /dev/null
+++ b/remoroo_monitor.py
@@ -0,0 +1,100 @@
+"""
+Runtime monitoring helper for Remoroo instrumentation.
+This module is injected into the user's repository during experimentation.
+It provides a safe, atomic way to emit metrics without race conditions.
+"""
+import os
+import json
+import uuid
+import time
+import sys
+from typing import Any, Optional
+
+class MetricEmitter:
+    """
+    Handles atomic emission of metrics to partial artifact files.
+    This avoids lock contention and race conditions when multiple processes
+    try to write to a single metrics.json file.
+    """
+    
+    def __init__(self, artifact_dir: Optional[str] = None):
+        """
+        Initialize the emitter.
+        
+        Args:
+            artifact_dir: Optional explicit path. If None, looks for REMOROO_ARTIFACTS_DIR
+                         env var, or falls back to 'artifacts' in current directory.
+        """
+        self.artifact_dir = (
+            artifact_dir 
+            or os.environ.get("REMOROO_ARTIFACTS_DIR") 
+            or os.path.join(os.getcwd(), "artifacts")
+        )
+        # Ensure it exists (safe mkdir)
+        try:
+            os.makedirs(self.artifact_dir, exist_ok=True)
+        except Exception:
+            pass
+            
+        self.pid = os.getpid()
+        self.process_uuid = str(uuid.uuid4())[:8]
+
+    def emit(self, name: str, value: Any, unit: str = "", source: str = "custom_instrumentation") -> bool:
+        """
+        Emit a single metric to a unique partial artifact file.
+        
+        Args:
+            name: Metric name
+            value: Metric value
+            unit: Optional unit string
+            source: Source identifier
+            
+        Returns:
+            bool: True if write succeeded, False otherwise.
+        """
+        try:
+            timestamp = time.time()
+            # Unique filename for this emission to guarantee atomicity
+            # format: partial_{timestamp}_{uuid}_{name}.json
+            # We include name in filename to make debugging easier, but uuid ensures uniqueness
+            safe_name = "".join(c for c in name if c.isalnum() or c in "._-")[:50]
+            filename = f"partial_{timestamp:.6f}_{self.process_uuid}_{safe_name}.json"
+            filepath = os.path.join(self.artifact_dir, filename)
+            
+            payload = {
+                "metric_name": name,
+                "value": value,
+                "unit": unit,
+                "source": source,
+                "timestamp": timestamp,
+                "pid": self.pid,
+                "process_uuid": self.process_uuid,
+                "version": "1.0" # schema version for partial artifacts
+            }
+            
+            # Atomic write pattern: write to temp then rename (if on POSIX)
+            # For simplicity in this injected helper, we just write a unique file.
+            # Since the filename includes random UUID time, collision is effectively impossible.
+            with open(filepath, "w", encoding="utf-8") as f:
+                json.dump(payload, f)
+                
+            return True
+        except Exception as e:
+            # Last resort stderr logging if emission fails
+            sys.stderr.write(f"[Remoroo] Failed to emit metric '{name}': {e}\n")
+            return False
+
+# Global instance for easy import usage
+_global_emitter = None
+
+def emit(name: str, value: Any, unit: str = "", source: str = "custom_instrumentation"):
+    """
+    Global convenience function.
+    Usage:
+        import monitor
+        monitor.emit("accuracy", 0.95)
+    """
+    global _global_emitter
+    if _global_emitter is None:
+        _global_emitter = MetricEmitter()
+    return _global_emitter.emit(name, value, unit, source)
diff --git a/resource_manager.py b/resource_manager.py
index e1d6ac1..794322c 100644
--- a/resource_manager.py
+++ b/resource_manager.py
@@ -1,97 +1,58 @@
-"""Resource manager with deadlock bug.
-
-This simulates a banking system where transfers between accounts
-can deadlock due to inconsistent lock ordering.
-
-The classic deadlock scenario:
-- Thread 1: locks Account A, then tries to lock Account B
-- Thread 2: locks Account B, then tries to lock Account A
-- Both threads wait forever = DEADLOCK
-
-The fix requires consistent lock ordering (e.g., always lock lower ID first).
-"""
-
 import threading
 import time
 import random
+import os
+import pathlib
+import json
 
 class Account:
     """Bank account with a lock for thread-safe operations."""
-    
     def __init__(self, account_id: int, balance: float = 1000.0):
         self.account_id = account_id
         self.balance = balance
         self.lock = threading.Lock()
-    
     def __repr__(self):
         return f"Account({self.account_id}, balance={self.balance:.2f})"
 
-
 class ResourceManager:
     """Manages transfers between accounts.
-    
-    BUG: Deadlock due to inconsistent lock ordering!
-    
-    When transferring from A to B:
-    - This code locks A first, then B
-    - But another thread transferring B to A locks B first, then A
-    - This causes deadlock!
-    
-    FIX: Always acquire locks in a consistent order (e.g., by account_id)
+    Uses a global lock to ensure atomicity and avoid deadlocks for high concurrency.
     """
-    
     def __init__(self):
         self.accounts = {}
         self.transfer_count = 0
         self.count_lock = threading.Lock()
-    
+        self.global_lock = threading.Lock()  # Global lock for all transfers
     def create_account(self, account_id: int, initial_balance: float = 1000.0) -> Account:
         account = Account(account_id, initial_balance)
         self.accounts[account_id] = account
         return account
-    
     def transfer(self, from_id: int, to_id: int, amount: float) -> bool:
         """Transfer amount from one account to another.
-        
-        BUG: Acquires locks in order of parameters, not by account_id!
-        This causes deadlock when two threads transfer in opposite directions.
+        Uses a global lock to avoid deadlocks and maximize throughput for small number of accounts.
         """
         if from_id == to_id:
             return False
-        
         from_account = self.accounts.get(from_id)
         to_account = self.accounts.get(to_id)
-        
         if not from_account or not to_account:
             return False
-        
-        # BUG: Lock ordering based on parameter order causes deadlock!
-        # Thread 1: transfer(A, B) -> locks A, then B
-        # Thread 2: transfer(B, A) -> locks B, then A
-        # DEADLOCK!
-        
-        with from_account.lock:  # BUG: Should use consistent ordering!
-            # Small delay to increase chance of deadlock
+        # Use a global lock to ensure atomicity and avoid deadlocks
+        with self.global_lock:
+            # Small delay to simulate contention (for testing, keep as in original)
             time.sleep(0.001)
-            
-            with to_account.lock:  # BUG: This is where deadlock occurs
-                if from_account.balance >= amount:
-                    from_account.balance -= amount
-                    to_account.balance += amount
-                    
-                    with self.count_lock:
-                        self.transfer_count += 1
-                    
-                    return True
-        
+            if from_account.balance >= amount:
+                from_account.balance -= amount
+                to_account.balance += amount
+                with self.count_lock:
+                    self.transfer_count += 1
+                return True
         return False
-    
     def get_transfer_count(self) -> int:
         with self.count_lock:
             return self.transfer_count
 
-
-def worker(manager: ResourceManager, num_transfers: int, account_ids: list):
+def worker(manager: 'ResourceManager', num_transfers: int, account_ids: list):
     """Worker that performs random transfers."""
     for _ in range(num_transfers):
         from_id = random.choice(account_ids)
@@ -99,59 +60,100 @@ def worker(manager: ResourceManager, num_transfers: int, account_ids: list):
         if from_id != to_id:
             manager.transfer(from_id, to_id, random.uniform(1, 10))
 
+def emit_metrics_artifacts(completed_transfers):
+    import time
+    # Sentinel marker: REMOROO_METRICS_EMIT_BLOCK
+    phase = os.environ.get("REMOROO_METRICS_PHASE", "current")
+    artifacts_dir = os.environ.get("REMOROO_ARTIFACTS_DIR", None)
+    if not artifacts_dir:
+        print("DEBUG: REMOROO_ARTIFACTS_DIR not set, skipping metrics emission.")
+        return
+    artifacts_dir = pathlib.Path(artifacts_dir)
+    artifacts_dir.mkdir(exist_ok=True)
+    metric_name = "completed_transfers"
+    metric_value = completed_transfers
+    metrics_with_units = {metric_name: {"value": metric_value, "unit": ""}}
+    created_at = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
+    source = "resource_manager.py"
+    targets = []
+    if phase == "baseline":
+        targets.append(artifacts_dir / "baseline_metrics.json")
+    else:
+        targets.append(artifacts_dir / "current_metrics.json")
+    targets.append(artifacts_dir / "metrics.json")
+    for target_file in targets:
+        print(f"DEBUG: Checking {target_file}, exists={target_file.exists()}")
+        data = {}
+        if target_file.exists():
+            try:
+                with open(target_file, "r") as f:
+                    data = json.load(f)
+                    print(f"DEBUG: Read merged keys: {list(data.get('metrics', {}).keys())}")
+            except Exception as e:
+                print(f"DEBUG: Failed to read {target_file}: {e}")
+                data = {}
+        # Initialize A-pattern if needed
+        if "metrics" not in data:
+            data["metrics"] = {}
+            data["version"] = 1
+            data["phase"] = phase
+            data["created_at"] = created_at
+            data["source"] = source
+            data["metrics_with_units"] = {}
+        # Update metric
+        data["metrics"][metric_name] = metric_value
+        data["metrics_with_units"][metric_name] = {"value": metric_value, "unit": ""}
+        # Also update baseline_metrics key if in baseline phase (for metrics.json compatibility)
+        if phase == "baseline" and target_file.name == "metrics.json":
+            if "baseline_metrics" not in data:
+                data["baseline_metrics"] = {}
+            data["baseline_metrics"][metric_name] = metric_value
+        print(f"DEBUG: Writing {target_file.name} keys: {list(data['metrics'].keys())}")
+        with open(target_file, "w") as f:
+            json.dump(data, f, indent=2)
 
 def main():
     """Test the resource manager with concurrent transfers."""
     random.seed(42)
-    
     manager = ResourceManager()
-    
     # Create accounts
     num_accounts = 5
     for i in range(num_accounts):
         manager.create_account(i, 10000.0)
-    
     account_ids = list(range(num_accounts))
-    
     # Spawn workers
     num_workers = 10
     transfers_per_worker = 100
     expected_transfers = num_workers * transfers_per_worker
-    
     print(f"Starting {num_workers} workers, each doing {transfers_per_worker} transfers")
     print(f"Expected total transfers: {expected_transfers}")
     print("(If this hangs, there's a deadlock!)\n")
-    
     threads = []
     start_time = time.time()
-    
     for i in range(num_workers):
         t = threading.Thread(target=worker, args=(manager, transfers_per_worker, account_ids))
         threads.append(t)
         t.start()
-    
     # Wait with timeout
     timeout = 30  # seconds
     for t in threads:
         t.join(timeout=timeout)
         if t.is_alive():
             print("DEADLOCK DETECTED! Thread did not complete.")
-            print(f"completed_transfers: {manager.get_transfer_count()}")
+            completed = manager.get_transfer_count()
+            print(f"completed_transfers: {completed}")
+            emit_metrics_artifacts(completed)
             return
-    
     elapsed = time.time() - start_time
     completed = manager.get_transfer_count()
-    
     print(f"Completed in {elapsed:.2f} seconds")
     print(f"completed_transfers: {completed}")
-    
+    emit_metrics_artifacts(completed)
     # Note: Not all transfers succeed (insufficient balance), but should be close
     if completed >= expected_transfers * 0.8:  # At least 80% should complete
         print(f"\nSUCCESS: {completed} transfers completed without deadlock!")
     else:
         print(f"\nFAILED: Only {completed}/{expected_transfers} transfers completed")
 
-
 if __name__ == "__main__":
     main()
-
