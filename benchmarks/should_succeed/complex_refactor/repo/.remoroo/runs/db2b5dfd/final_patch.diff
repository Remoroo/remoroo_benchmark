diff --git a/data_transformer.py b/data_transformer.py
new file mode 100644
index 0000000..9d7b02c
--- /dev/null
+++ b/data_transformer.py
@@ -0,0 +1,22 @@
+import re
+
+class DataTransformer:
+    """
+    Handles transformation logic for data items.
+    """
+    def transform(self, item, config):
+        # Normalize value
+        val = str(item['value']).strip().lower()
+        # Remove special chars
+        val = re.sub(r'[^a-z0-9\s]', '', val)
+        # Truncate if too long (legacy rule)
+        if len(val) > 100:
+            val = val[:100]
+        # Enrich data
+        processed_item = {
+            'id': item['id'],
+            'clean_value': val,
+            'status': 'processed',
+            'version': config.get('version', 1)
+        }
+        return processed_item
diff --git a/data_validator.py b/data_validator.py
new file mode 100644
index 0000000..a73dcec
--- /dev/null
+++ b/data_validator.py
@@ -0,0 +1,15 @@
+class DataValidator:
+    """
+    Handles validation logic for data items.
+    """
+    def is_valid(self, item):
+        if not isinstance(item, dict):
+            print(f"Skipping invalid item: {item}")
+            return False
+        if 'id' not in item or 'value' not in item:
+            print(f"Skipping missing fields: {item}")
+            return False
+        if not isinstance(item['id'], int) or item['id'] < 0:
+            print(f"Invalid ID: {item['id']}")
+            return False
+        return True
diff --git a/legacy_processor.py b/legacy_processor.py
index e2d617a..0a8812b 100644
--- a/legacy_processor.py
+++ b/legacy_processor.py
@@ -1,49 +1,98 @@
 import json
-import re
+import os
+import pathlib
+import time
+import sys
+from data_validator import DataValidator
+from data_transformer import DataTransformer
 
 class LegacyProcessor:
     def __init__(self, config):
         self.config = config
         self.processed_count = 0
+        self.validator = DataValidator()
+        self.transformer = DataTransformer()
 
     def process_data(self, data_list):
         results = []
         for item in data_list:
-            # --- VALIDATION LOGIC (Mixed in) ---
-            if not isinstance(item, dict):
-                print(f"Skipping invalid item: {item}")
+            if not self.validator.is_valid(item):
                 continue
-            
-            if 'id' not in item or 'value' not in item:
-                print(f"Skipping missing fields: {item}")
-                continue
-            
-            if not isinstance(item['id'], int) or item['id'] < 0:
-                print(f"Invalid ID: {item['id']}")
-                continue
-
-            # --- TRANSFORMATION LOGIC (Mixed in) ---
-            # Normalize value
-            val = str(item['value']).strip().lower()
-            
-            # Remove special chars
-            val = re.sub(r'[^a-z0-9\s]', '', val)
-            
-            # Truncate if too long (legacy rule)
-            if len(val) > 100:
-                val = val[:100]
-                
-            # Enrich data
-            processed_item = {
-                'id': item['id'],
-                'clean_value': val,
-                'status': 'processed',
-                'version': self.config.get('version', 1)
-            }
+            processed_item = self.transformer.transform(item, self.config)
             results.append(processed_item)
             self.processed_count += 1
-            
         return results
 
     def get_stats(self):
         return {"count": self.processed_count}
+
+def _emit_remoroo_metrics(tests_passed):
+    # Use env var for artifacts dir
+    import os, pathlib, json, time
+    artifacts_dir = pathlib.Path(os.environ["REMOROO_ARTIFACTS_DIR"])
+    artifacts_dir.mkdir(exist_ok=True)
+    phase = os.environ.get("REMOROO_METRICS_PHASE", "current")
+    metric_name = "tests_passed"
+    metric_value = tests_passed
+    targets = []
+    if phase == "baseline":
+        targets.append(artifacts_dir / "baseline_metrics.json")
+    else:
+        targets.append(artifacts_dir / "current_metrics.json")
+    targets.append(artifacts_dir / "metrics.json")
+    for target_file in targets:
+        print(f"DEBUG: Processing target {target_file}")
+        data = {}
+        if target_file.exists():
+            try:
+                with open(target_file, "r") as f:
+                    data = json.load(f)
+                    print(f"DEBUG: Loaded keys from {target_file.name}: {list(data.get('metrics', {}).keys())}")
+            except Exception as e:
+                print(f"DEBUG: Failed to read {target_file}: {e}")
+                data = {}
+        # Initialize A-pattern if needed
+        if "metrics" not in data:
+            data["metrics"] = {}
+            data["version"] = 1
+            data["phase"] = phase
+            data["created_at"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
+        # Update metric
+        data["metrics"][metric_name] = metric_value
+        # Also update baseline_metrics key if in baseline phase (for metrics.json compatibility)
+        if phase == "baseline" and target_file.name == "metrics.json":
+            if "baseline_metrics" not in data:
+                data["baseline_metrics"] = {}
+            data["baseline_metrics"][metric_name] = metric_value
+        print(f"DEBUG: Writing {target_file.name} keys: {list(data['metrics'].keys())}")
+        with open(target_file, "w") as f:
+            json.dump(data, f, indent=2)
+
+def _run_and_emit():
+    # Simulate a test run: process some data and check correctness
+    # This is a minimal test harness for metrics emission
+    config = {"version": 1}
+    processor = LegacyProcessor(config)
+    # Test data: valid and invalid
+    data = [
+        {"id": 1, "value": "Hello!"},
+        {"id": 2, "value": "World@@@"},
+        {"id": -1, "value": "bad id"},
+        {"id": 3, "value": "A"*120},
+        {"id": "x", "value": "bad type"},
+        {"id": 4, "value": "  Clean  "},
+        {"value": "missing id"},
+        {"id": 5, "value": "OK"}
+    ]
+    results = processor.process_data(data)
+    # Check that only valid items are processed
+    expected_ids = {1,2,3,4,5}
+    processed_ids = {item['id'] for item in results}
+    # Only positive int ids, with both fields, should be processed
+    tests_passed = processed_ids == {1,2,3,4,5}
+    _emit_remoroo_metrics(tests_passed)
+    # Print for legacy behavior
+    print(f"Tests passed: {tests_passed}")
+
+if __name__ == "__main__":
+    _run_and_emit()
diff --git a/remoroo_monitor.py b/remoroo_monitor.py
new file mode 100644
index 0000000..e593c4a
--- /dev/null
+++ b/remoroo_monitor.py
@@ -0,0 +1,100 @@
+"""
+Runtime monitoring helper for Remoroo instrumentation.
+This module is injected into the user's repository during experimentation.
+It provides a safe, atomic way to emit metrics without race conditions.
+"""
+import os
+import json
+import uuid
+import time
+import sys
+from typing import Any, Optional
+
+class MetricEmitter:
+    """
+    Handles atomic emission of metrics to partial artifact files.
+    This avoids lock contention and race conditions when multiple processes
+    try to write to a single metrics.json file.
+    """
+    
+    def __init__(self, artifact_dir: Optional[str] = None):
+        """
+        Initialize the emitter.
+        
+        Args:
+            artifact_dir: Optional explicit path. If None, looks for REMOROO_ARTIFACTS_DIR
+                         env var, or falls back to 'artifacts' in current directory.
+        """
+        self.artifact_dir = (
+            artifact_dir 
+            or os.environ.get("REMOROO_ARTIFACTS_DIR") 
+            or os.path.join(os.getcwd(), "artifacts")
+        )
+        # Ensure it exists (safe mkdir)
+        try:
+            os.makedirs(self.artifact_dir, exist_ok=True)
+        except Exception:
+            pass
+            
+        self.pid = os.getpid()
+        self.process_uuid = str(uuid.uuid4())[:8]
+
+    def emit(self, name: str, value: Any, unit: str = "", source: str = "custom_instrumentation") -> bool:
+        """
+        Emit a single metric to a unique partial artifact file.
+        
+        Args:
+            name: Metric name
+            value: Metric value
+            unit: Optional unit string
+            source: Source identifier
+            
+        Returns:
+            bool: True if write succeeded, False otherwise.
+        """
+        try:
+            timestamp = time.time()
+            # Unique filename for this emission to guarantee atomicity
+            # format: partial_{timestamp}_{uuid}_{name}.json
+            # We include name in filename to make debugging easier, but uuid ensures uniqueness
+            safe_name = "".join(c for c in name if c.isalnum() or c in "._-")[:50]
+            filename = f"partial_{timestamp:.6f}_{self.process_uuid}_{safe_name}.json"
+            filepath = os.path.join(self.artifact_dir, filename)
+            
+            payload = {
+                "metric_name": name,
+                "value": value,
+                "unit": unit,
+                "source": source,
+                "timestamp": timestamp,
+                "pid": self.pid,
+                "process_uuid": self.process_uuid,
+                "version": "1.0" # schema version for partial artifacts
+            }
+            
+            # Atomic write pattern: write to temp then rename (if on POSIX)
+            # For simplicity in this injected helper, we just write a unique file.
+            # Since the filename includes random UUID time, collision is effectively impossible.
+            with open(filepath, "w", encoding="utf-8") as f:
+                json.dump(payload, f)
+                
+            return True
+        except Exception as e:
+            # Last resort stderr logging if emission fails
+            sys.stderr.write(f"[Remoroo] Failed to emit metric '{name}': {e}\n")
+            return False
+
+# Global instance for easy import usage
+_global_emitter = None
+
+def emit(name: str, value: Any, unit: str = "", source: str = "custom_instrumentation"):
+    """
+    Global convenience function.
+    Usage:
+        import monitor
+        monitor.emit("accuracy", 0.95)
+    """
+    global _global_emitter
+    if _global_emitter is None:
+        _global_emitter = MetricEmitter()
+    return _global_emitter.emit(name, value, unit, source)
