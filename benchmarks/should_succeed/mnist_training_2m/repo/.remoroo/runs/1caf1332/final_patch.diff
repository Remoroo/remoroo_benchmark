diff --git a/mnist_classifier.py b/mnist_classifier.py
new file mode 100644
index 0000000..269ff22
--- /dev/null
+++ b/mnist_classifier.py
@@ -0,0 +1,170 @@
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torch.utils.data import DataLoader
+from torchvision import datasets, transforms
+import argparse
+import os
+import time
+import json
+
+ARTIFACTS_DIR = 'artifacts'
+METRICS_PATH = os.path.join(ARTIFACTS_DIR, 'metrics.json')
+METRICS_PROVENANCE_PATH = os.path.join(ARTIFACTS_DIR, 'metrics_provenance.json')
+MODEL_PATH = os.path.join(ARTIFACTS_DIR, 'mnist_cnn.pt')
+
+# Ensure artifacts directory exists
+def ensure_artifacts_dir():
+    if not os.path.exists(ARTIFACTS_DIR):
+        os.makedirs(ARTIFACTS_DIR)
+
+def save_metrics(metrics, provenance):
+    ensure_artifacts_dir()
+    with open(METRICS_PATH, 'w') as f:
+        json.dump(metrics, f, indent=2)
+    with open(METRICS_PROVENANCE_PATH, 'w') as f:
+        json.dump(provenance, f, indent=2)
+
+class SimpleCNN(nn.Module):
+    def __init__(self):
+        super(SimpleCNN, self).__init__()
+        self.conv1 = nn.Conv2d(1, 32, 3, 1)
+        self.conv2 = nn.Conv2d(32, 64, 3, 1)
+        self.dropout1 = nn.Dropout(0.25)
+        self.dropout2 = nn.Dropout(0.5)
+        self.fc1 = nn.Linear(9216, 128)
+        self.fc2 = nn.Linear(128, 10)
+
+    def forward(self, x):
+        x = self.conv1(x)
+        x = torch.relu(x)
+        x = self.conv2(x)
+        x = torch.relu(x)
+        x = torch.max_pool2d(x, 2)
+        x = self.dropout1(x)
+        x = torch.flatten(x, 1)
+        x = self.fc1(x)
+        x = torch.relu(x)
+        x = self.dropout2(x)
+        x = self.fc2(x)
+        output = torch.log_softmax(x, dim=1)
+        return output
+
+def get_data_loaders(batch_size=64):
+    transform = transforms.Compose([
+        transforms.ToTensor(),
+        transforms.Normalize((0.1307,), (0.3081,))
+    ])
+    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
+    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)
+    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
+    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=2)
+    return train_loader, test_loader
+
+def train(model, device, train_loader, optimizer, criterion, epoch):
+    model.train()
+    for batch_idx, (data, target) in enumerate(train_loader):
+        data, target = data.to(device), target.to(device)
+        optimizer.zero_grad()
+        output = model(data)
+        loss = criterion(output, target)
+        loss.backward()
+        optimizer.step()
+        if batch_idx % 100 == 0:
+            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}' +
+                  f' ({100. * batch_idx / len(train_loader):.0f}%)]\tLoss: {loss.item():.6f}')
+
+def test(model, device, test_loader):
+    model.eval()
+    test_loss = 0
+    correct = 0
+    criterion = nn.NLLLoss()
+    with torch.no_grad():
+        for data, target in test_loader:
+            data, target = data.to(device), target.to(device)
+            output = model(data)
+            test_loss += criterion(output, target).item() * data.size(0)
+            pred = output.argmax(dim=1, keepdim=True)
+            correct += pred.eq(target.view_as(pred)).sum().item()
+    test_loss /= len(test_loader.dataset)
+    accuracy = correct / len(test_loader.dataset)
+    print(f'\nValidation set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}' +
+          f' ({100. * accuracy:.2f}%)\n')
+    return accuracy, test_loss
+
+def measure_inference_time(model, device, test_loader, n_batches=10):
+    model.eval()
+    times = []
+    with torch.no_grad():
+        for i, (data, _) in enumerate(test_loader):
+            if i >= n_batches:
+                break
+            data = data.to(device)
+            start = time.time()
+            _ = model(data)
+            end = time.time()
+            times.append((end - start) * 1000)  # ms
+    avg_time = sum(times) / len(times) if times else 0.0
+    return avg_time
+
+def main():
+    parser = argparse.ArgumentParser(description='MNIST CNN Classifier')
+    parser.add_argument('--train', action='store_true', help='Train the model')
+    parser.add_argument('--evaluate', action='store_true', help='Evaluate the model')
+    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs to train')
+    parser.add_argument('--batch-size', type=int, default=64, help='Batch size for training')
+    parser.add_argument('--lr', type=float, default=1.0, help='Learning rate')
+    parser.add_argument('--no-cuda', action='store_true', default=False, help='Disable CUDA')
+    args = parser.parse_args()
+
+    use_cuda = torch.cuda.is_available() and not args.no_cuda
+    device = torch.device('cuda' if use_cuda else 'cpu')
+    print(f'Using device: {device}')
+
+    train_loader, test_loader = get_data_loaders(batch_size=args.batch_size)
+    model = SimpleCNN().to(device)
+    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)
+    criterion = nn.NLLLoss()
+
+    if args.train:
+        best_acc = 0.0
+        for epoch in range(1, args.epochs + 1):
+            train(model, device, train_loader, optimizer, criterion, epoch)
+            val_acc, val_loss = test(model, device, test_loader)
+            if val_acc > best_acc:
+                best_acc = val_acc
+                ensure_artifacts_dir()
+                torch.save(model.state_dict(), MODEL_PATH)
+        print(f'Best validation accuracy: {best_acc:.4f}')
+
+    if args.evaluate:
+        if not os.path.exists(MODEL_PATH):
+            print(f'Model file {MODEL_PATH} not found. Please train first.')
+            return
+        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
+        val_acc, val_loss = test(model, device, test_loader)
+        inference_time = measure_inference_time(model, device, test_loader, n_batches=10)
+        print(f'validation_accuracy: {val_acc:.4f}')
+        print(f'inference_time: {inference_time:.2f} ms')
+        metrics = {
+            'validation_accuracy': val_acc,
+            'inference_time': inference_time
+        }
+        provenance = {
+            'validation_accuracy': {
+                'source': 'test() in mnist_classifier.py',
+                'inputs': ['MNIST test set'],
+                'unit': 'fraction',
+                'conversion': 'none'
+            },
+            'inference_time': {
+                'source': 'measure_inference_time() in mnist_classifier.py',
+                'inputs': ['MNIST test set, 10 batches'],
+                'unit': 'milliseconds',
+                'conversion': 'none'
+            }
+        }
+        save_metrics(metrics, provenance)
+
+if __name__ == '__main__':
+    main()
