diff --git a/cache.py b/cache.py
index cf33e60..64afaa9 100644
--- a/cache.py
+++ b/cache.py
@@ -1,47 +1,46 @@
-"""Cache implementation with memory leak."""
+"""Cache implementation with LRU eviction (max_size=100)."""
 
 import time
 from collections import OrderedDict
+import os
+import pathlib
+import json
 
 import torch
 
 class Cache:
-    """A simple cache that stores computed results.
-    
-    BUG: This cache grows unbounded! It never evicts old entries,
-    causing memory to grow indefinitely with usage.
-    
-    GOAL: Implement LRU (Least Recently Used) eviction with max_size=100
-    """
-    
+    """A simple LRU cache that stores computed results with a max size."""
     def __init__(self, max_size=100):
         self.max_size = max_size
-        self.cache = {}  # BUG: Using regular dict, not tracking access order
+        self.cache = OrderedDict()  # Use OrderedDict for LRU
         self.hits = 0
         self.misses = 0
-    
+
     def get(self, key):
-        """Get a value from cache.
-        
-        BUG: Doesn't update access time for LRU tracking.
-        """
+        """Get a value from cache. Updates access order for LRU."""
         if key in self.cache:
             self.hits += 1
+            # Move key to end to mark as recently used
+            self.cache.move_to_end(key)
             return self.cache[key]
         self.misses += 1
         return None
-    
+
     def set(self, key, value):
-        """Set a value in cache.
-        
-        BUG: Never checks max_size or evicts old entries!
-        This causes unbounded memory growth.
-        """
-        self.cache[key] = value  # BUG: No eviction logic!
-    
+        """Set a value in cache. Evicts least recently used if over max_size."""
+        if key in self.cache:
+            # Update value and mark as recently used
+            self.cache.move_to_end(key)
+            self.cache[key] = value
+        else:
+            self.cache[key] = value
+            if len(self.cache) > self.max_size:
+                # Pop the least recently used item (first item)
+                self.cache.popitem(last=False)
+
     def size(self):
         return len(self.cache)
-    
+
     def stats(self):
         total = self.hits + self.misses
         hit_rate = self.hits / total if total > 0 else 0
@@ -73,38 +72,87 @@ def process_with_cache(cache, items):
     return results
 
 
+def emit_metrics_artifacts(metrics_dict, phase):
+    import time
+    # Use REMOROO_ARTIFACTS_DIR for all artifact paths
+    artifacts_dir = pathlib.Path(os.environ["REMOROO_ARTIFACTS_DIR"])
+    artifacts_dir.mkdir(exist_ok=True)
+    # Determine targets
+    targets = []
+    if phase == "baseline":
+        targets.append(artifacts_dir / "baseline_metrics.json")
+    else:
+        targets.append(artifacts_dir / "current_metrics.json")
+    targets.append(artifacts_dir / "metrics.json")
+    for target_file in targets:
+        print(f"DEBUG: Checking {target_file}, exists={target_file.exists()}")
+        data = {}
+        if target_file.exists():
+            try:
+                with open(target_file, "r") as f:
+                    data = json.load(f)
+                    print(f"DEBUG: Read merged keys: {list(data.get('metrics', {}).keys())}")
+            except Exception as e:
+                print(f"DEBUG: Failed to read {target_file}: {e}")
+                data = {}
+        # Initialize A-pattern if needed
+        if "metrics" not in data:
+            data["metrics"] = {}
+            data["version"] = 1
+            data["phase"] = phase
+            data["created_at"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
+        # Update metrics
+        for k, v in metrics_dict.items():
+            data["metrics"][k] = v
+        # Also update baseline_metrics key if in baseline phase (for metrics.json compatibility)
+        if phase == "baseline" and target_file.name == "metrics.json":
+            if "baseline_metrics" not in data:
+                data["baseline_metrics"] = {}
+            for k, v in metrics_dict.items():
+                data["baseline_metrics"][k] = v
+        print(f"DEBUG: Writing {target_file.name} keys: {list(data['metrics'].keys())}")
+        with open(target_file, "w") as f:
+            json.dump(data, f, indent=2)
+
+
 def main():
     """Test the cache with many items."""
     cache = Cache(max_size=100)
-    
+
     # Process 1000 different items
     num_items = 1000
     items = list(range(num_items))
-    
+
     print(f"Processing {num_items} unique items with cache (max_size={cache.max_size})")
-    
+
     start_time = time.time()
     results = process_with_cache(cache, items)
     elapsed = time.time() - start_time
-    
+
     stats = cache.stats()
     print(f"\nCompleted in {elapsed:.2f} seconds")
     print(f"Cache stats: {stats}")
-    
+
     # Check if memory is stable (cache respects max_size)
     memory_stable = cache.size() <= cache.max_size
-    
+
     print(f"\nCache size: {cache.size()}")
     print(f"Max allowed: {cache.max_size}")
     print(f"memory_stable: {memory_stable}")
-    
+
     if memory_stable:
         print("\nSUCCESS: Cache properly evicts old entries!")
     else:
         print(f"\nFAILED: Cache size ({cache.size()}) exceeds max_size ({cache.max_size})")
         print("Memory leak detected - cache grows unbounded!")
 
+    # --- METRICS ARTIFACT EMISSION ---
+    # Only emit if REMOROO_ARTIFACTS_DIR is set (engine context)
+    if "REMOROO_ARTIFACTS_DIR" in os.environ:
+        phase = os.environ.get("REMOROO_METRICS_PHASE", "current")
+        metrics_dict = {"memory_stable": memory_stable}
+        emit_metrics_artifacts(metrics_dict, phase)
+
 
 if __name__ == "__main__":
     main()
-
diff --git a/remoroo_monitor.py b/remoroo_monitor.py
new file mode 100644
index 0000000..e593c4a
--- /dev/null
+++ b/remoroo_monitor.py
@@ -0,0 +1,100 @@
+"""
+Runtime monitoring helper for Remoroo instrumentation.
+This module is injected into the user's repository during experimentation.
+It provides a safe, atomic way to emit metrics without race conditions.
+"""
+import os
+import json
+import uuid
+import time
+import sys
+from typing import Any, Optional
+
+class MetricEmitter:
+    """
+    Handles atomic emission of metrics to partial artifact files.
+    This avoids lock contention and race conditions when multiple processes
+    try to write to a single metrics.json file.
+    """
+    
+    def __init__(self, artifact_dir: Optional[str] = None):
+        """
+        Initialize the emitter.
+        
+        Args:
+            artifact_dir: Optional explicit path. If None, looks for REMOROO_ARTIFACTS_DIR
+                         env var, or falls back to 'artifacts' in current directory.
+        """
+        self.artifact_dir = (
+            artifact_dir 
+            or os.environ.get("REMOROO_ARTIFACTS_DIR") 
+            or os.path.join(os.getcwd(), "artifacts")
+        )
+        # Ensure it exists (safe mkdir)
+        try:
+            os.makedirs(self.artifact_dir, exist_ok=True)
+        except Exception:
+            pass
+            
+        self.pid = os.getpid()
+        self.process_uuid = str(uuid.uuid4())[:8]
+
+    def emit(self, name: str, value: Any, unit: str = "", source: str = "custom_instrumentation") -> bool:
+        """
+        Emit a single metric to a unique partial artifact file.
+        
+        Args:
+            name: Metric name
+            value: Metric value
+            unit: Optional unit string
+            source: Source identifier
+            
+        Returns:
+            bool: True if write succeeded, False otherwise.
+        """
+        try:
+            timestamp = time.time()
+            # Unique filename for this emission to guarantee atomicity
+            # format: partial_{timestamp}_{uuid}_{name}.json
+            # We include name in filename to make debugging easier, but uuid ensures uniqueness
+            safe_name = "".join(c for c in name if c.isalnum() or c in "._-")[:50]
+            filename = f"partial_{timestamp:.6f}_{self.process_uuid}_{safe_name}.json"
+            filepath = os.path.join(self.artifact_dir, filename)
+            
+            payload = {
+                "metric_name": name,
+                "value": value,
+                "unit": unit,
+                "source": source,
+                "timestamp": timestamp,
+                "pid": self.pid,
+                "process_uuid": self.process_uuid,
+                "version": "1.0" # schema version for partial artifacts
+            }
+            
+            # Atomic write pattern: write to temp then rename (if on POSIX)
+            # For simplicity in this injected helper, we just write a unique file.
+            # Since the filename includes random UUID time, collision is effectively impossible.
+            with open(filepath, "w", encoding="utf-8") as f:
+                json.dump(payload, f)
+                
+            return True
+        except Exception as e:
+            # Last resort stderr logging if emission fails
+            sys.stderr.write(f"[Remoroo] Failed to emit metric '{name}': {e}\n")
+            return False
+
+# Global instance for easy import usage
+_global_emitter = None
+
+def emit(name: str, value: Any, unit: str = "", source: str = "custom_instrumentation"):
+    """
+    Global convenience function.
+    Usage:
+        import monitor
+        monitor.emit("accuracy", 0.95)
+    """
+    global _global_emitter
+    if _global_emitter is None:
+        _global_emitter = MetricEmitter()
+    return _global_emitter.emit(name, value, unit, source)
